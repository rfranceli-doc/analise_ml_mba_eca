{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ğŸ¯ RegressÃ£o LogÃ­stica â€” Churn do Produto A\n",
    "**PÃ³s-GraduaÃ§Ã£o BI & Analytics Â· ML Aplicado a DecisÃµes de NegÃ³cio**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¡ Sinal de NegÃ³cio\n",
    "> O grÃ¡fico de vendas mostrou que o **Produto A estabilizou** enquanto o **Produto B cresceu 90%** em 17 meses.  \n",
    "> Isso gerou a pergunta: **quais clientes reduziram as compras do Produto A?**  \n",
    "> Queremos identificar esses clientes *antes* que o churn seja irreversÃ­vel.\n",
    "\n",
    "### ğŸ—ï¸ O que vamos construir\n",
    "Um modelo de **classificaÃ§Ã£o binÃ¡ria** que prevÃª a probabilidade de um cliente ter reduzido compras â‰¥ 20%.  \n",
    "A saÃ­da Ã© uma **lista ranqueada por risco** para o time comercial acionar.\n",
    "\n",
    "---\n",
    "### Roteiro\n",
    "1. Importar e explorar a base\n",
    "2. PrÃ©-processamento e engenharia de features\n",
    "3. Treinar o modelo\n",
    "4. Avaliar (Matriz de ConfusÃ£o, AUC-ROC)\n",
    "5. Expected Maximum Profit (EMP)\n",
    "6. Interpretar coeficientes e Odds Ratio\n",
    "7. Gerar lista acionÃ¡vel para o CRM\n",
    "8. Comparar com XGBoost (relaÃ§Ãµes nÃ£o-lineares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: imports e configuraÃ§Ã£o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, accuracy_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams.update({'figure.dpi': 120, 'font.family': 'DejaVu Sans',\n",
    "                     'axes.spines.top': False, 'axes.spines.right': False})\n",
    "print('âœ… Bibliotecas carregadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: carregar dados â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Altere a URL abaixo para o raw link do seu GitHub\n",
    "URL = 'https://raw.githubusercontent.com/SEU_USUARIO/ml-bi-analytics-aula/main/data/clientes_produto_A.csv'\n",
    "\n",
    "# Se estiver rodando localmente:\n",
    "# df = pd.read_csv('clientes_produto_A.csv')\n",
    "\n",
    "df = pd.read_csv(URL)\n",
    "print(f'Shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_title",
   "metadata": {},
   "source": ["## 1. ExploraÃ§Ã£o da Base (EDA)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda_overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: visÃ£o geral â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('=== TIPOS E MISSINGS ===')\n",
    "info = pd.DataFrame({\n",
    "    'dtype': df.dtypes,\n",
    "    'missing': df.isna().sum(),\n",
    "    'missing_%': (df.isna().mean() * 100).round(1)\n",
    "})\n",
    "print(info.to_string())\n",
    "\n",
    "print('\\n=== TARGET ===')\n",
    "vc = df['reduziu_compras'].value_counts()\n",
    "print(f'NÃ£o reduziu (0): {vc[0]:,} ({vc[0]/len(df):.1%})')\n",
    "print(f'Reduziu    (1): {vc[1]:,} ({vc[1]/len(df):.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: distribuiÃ§Ãµes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "num_cols = ['var_compras_6m','freq_compras_trim','ticket_medio_ratio',\n",
    "            'tempo_cliente_anos','nps_score','vendedor_rotatividade',\n",
    "            'segmento_A','canal_digital']\n",
    "\n",
    "cores = ['#c84b2f','#2f6ec8']\n",
    "for i, col in enumerate(num_cols):\n",
    "    for val, cor in zip([0,1], cores):\n",
    "        subset = df[df['reduziu_compras'] == val][col].dropna()\n",
    "        axes[i].hist(subset, bins=25, alpha=0.55, color=cor,\n",
    "                     label=f'churn={val}', density=True)\n",
    "    axes[i].set_title(col, fontsize=9)\n",
    "    axes[i].legend(fontsize=7)\n",
    "\n",
    "plt.suptitle('DistribuiÃ§Ã£o das Features por Classe de Churn', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preproc_title",
   "metadata": {},
   "source": ["## 2. PrÃ©-Processamento"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preproc_encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: encoding de variÃ¡veis categÃ³ricas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_model = df.copy()\n",
    "\n",
    "# Dummies para regiÃ£o e porte (drop_first evita multicolinearidade perfeita)\n",
    "df_model = pd.get_dummies(df_model, columns=['regiao', 'porte'], drop_first=True)\n",
    "\n",
    "# Remover colunas que nÃ£o entram no modelo\n",
    "drop_cols = ['id_cliente', 'segmento', 'prob_churn_real']\n",
    "df_model  = df_model.drop(columns=[c for c in drop_cols if c in df_model.columns])\n",
    "\n",
    "print('Colunas apÃ³s encoding:', df_model.shape[1])\n",
    "print(df_model.dtypes.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preproc_missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Tratamento de missing values   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dica: use SimpleImputer com strategy='median' para nps_score e ticket_medio_ratio\n",
    "# Lembre: imputar ANTES de separar treino/teste ou DEPOIS?\n",
    "\n",
    "# Separar X e y\n",
    "TARGET = 'reduziu_compras'\n",
    "X = df_model.drop(columns=[TARGET])\n",
    "y = df_model[TARGET]\n",
    "\n",
    "# Sua resposta:\n",
    "imputer = SimpleImputer(strategy=___)\n",
    "X_imp   = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "print(f'Missing apÃ³s imputaÃ§Ã£o: {X_imp.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preproc_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Split treino/teste + normalizaÃ§Ã£o   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dica: use train_test_split com test_size=0.2, stratify=y, random_state=42\n",
    "# Dica: StandardScaler â€” fit no treino, transform em ambos\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ___, ___, test_size=___, stratify=___, random_state=42\n",
    ")\n",
    "\n",
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = ___\n",
    "\n",
    "print(f'Treino: {X_train.shape} | Teste: {X_test.shape}')\n",
    "print(f'Churn no treino: {y_train.mean():.1%} | Churn no teste: {y_test.mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_title",
   "metadata": {},
   "source": ["## 3. Treinar o Modelo"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Treinar LogisticRegression  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dica: LogisticRegression(C=1.0, solver='lbfgs', max_iter=500, random_state=42)\n",
    "# Pergunta para o grupo: o que C controla?\n",
    "\n",
    "logit = LogisticRegression(C=___, solver=___, max_iter=500, random_state=42)\n",
    "logit.fit(___, ___)\n",
    "\n",
    "y_pred_logit  = logit.predict(X_test)\n",
    "y_proba_logit = logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('âœ… Modelo treinado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_title",
   "metadata": {},
   "source": ["## 4. AvaliaÃ§Ã£o do Modelo"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Matriz de confusÃ£o + mÃ©tricas  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "cm  = confusion_matrix(___, ___)\n",
    "auc = roc_auc_score(___, ___)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Heatmap da matriz de confusÃ£o\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, cm[i,j], ha='center', va='center',\n",
    "                     fontsize=14, fontweight='bold',\n",
    "                     color='white' if cm[i,j] > cm.max()/2 else 'black')\n",
    "axes[0].set(xticks=[0,1], yticks=[0,1],\n",
    "            xticklabels=['Pred 0','Pred 1'],\n",
    "            yticklabels=['Real 0','Real 1'],\n",
    "            title='Matriz de ConfusÃ£o')\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_logit)\n",
    "axes[1].plot(fpr, tpr, color='#c84b2f', lw=2, label=f'AUC = {auc:.3f}')\n",
    "axes[1].plot([0,1],[0,1],'--', color='#888', lw=1)\n",
    "axes[1].set(xlabel='FPR', ylabel='TPR', title='Curva ROC')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nAcurÃ¡cia : {accuracy_score(y_test, y_pred_logit):.3f}')\n",
    "print(f'AUC-ROC  : {auc:.3f}')\n",
    "print(f'F1-Score : {f1_score(y_test, y_pred_logit):.3f}')\n",
    "print(f'Recall   : {recall_score(y_test, y_pred_logit):.3f}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred_logit):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_discuss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: discussÃ£o FP vs FN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')\n",
    "print('â•‘  DEBATE: o que Ã© pior no nosso problema?             â•‘')\n",
    "print('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£')\n",
    "print('â•‘  Falso Negativo (FN): nÃ£o contactamos quem ia churnarâ•‘')\n",
    "print('â•‘  â†’ Custo = CLV perdido                               â•‘')\n",
    "print('â•‘                                                      â•‘')\n",
    "print('â•‘  Falso Positivo (FP): contactamos quem nÃ£o ia churnarâ•‘')\n",
    "print('â•‘  â†’ Custo = custo da intervenÃ§Ã£o (desconto, ligaÃ§Ã£o)  â•‘')\n",
    "print('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')\n",
    "print()\n",
    "print('Se CLV mÃ©dio = R$ 2.000 e custo de contato = R$ 50:')\n",
    "print(f'  FN: perdemos ~R$ {cm[1,0] * 2000:,}')\n",
    "print(f'  FP: gastamos ~R$ {cm[0,1] * 50:,}')\n",
    "print('\\nâ†’ Nesse contexto, minimizar FN (aumentar Recall) Ã© prioritÃ¡rio.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emp_title",
   "metadata": {},
   "source": ["## 5. Expected Maximum Profit (EMP)\n\n> O AUC-ROC nÃ£o nos diz *onde cortar* a probabilidade. O EMP encontra o threshold que **maximiza o lucro esperado** dado o custo de cada tipo de erro."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emp_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: EMP para churn e para default â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from scipy.special import expit\n",
    "\n",
    "def emp_churn(y_true, y_proba, clv=2000, custo_contato=50):\n",
    "    \"\"\"Threshold que maximiza lucro para retenÃ§Ã£o de clientes.\"\"\"\n",
    "    thresholds = np.linspace(0, 1, 300)\n",
    "    lucros = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        lucros.append(tp * (clv - custo_contato) - fp * custo_contato)\n",
    "    idx = np.argmax(lucros)\n",
    "    return thresholds, lucros, thresholds[idx], lucros[idx]\n",
    "\n",
    "def emp_default(y_true, y_proba, valor_emp=15000, taxa_rec=0.30, custo_rej=300):\n",
    "    \"\"\"Threshold Ã³timo para aprovaÃ§Ã£o de crÃ©dito.\"\"\"\n",
    "    thresholds = np.linspace(0, 1, 300)\n",
    "    lucros = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        lucros.append(tp * valor_emp * (1 - taxa_rec) - fp * custo_rej)\n",
    "    idx = np.argmax(lucros)\n",
    "    return thresholds, lucros, thresholds[idx], lucros[idx]\n",
    "\n",
    "thr_c, luc_c, t_ot_c, emp_c = emp_churn(y_test, y_proba_logit)\n",
    "thr_d, luc_d, t_ot_d, emp_d = emp_default(y_test, y_proba_logit)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "for ax, thr, luc, t_ot, emp_v, cor, titulo in [\n",
    "    (axes[0], thr_c, luc_c, t_ot_c, emp_c, '#c84b2f', 'EMP â€” Churn de Produto'),\n",
    "    (axes[1], thr_d, luc_d, t_ot_d, emp_d, '#2f6ec8', 'EMP â€” CrÃ©dito / Default')\n",
    "]:\n",
    "    ax.plot(thr, luc, color=cor, lw=2)\n",
    "    ax.axvline(t_ot, color='#1a1a1a', lw=1.3, linestyle='--',\n",
    "               label=f'Threshold Ã³timo: {t_ot:.2f}')\n",
    "    ax.scatter([t_ot], [emp_v], color='#1a1a1a', s=60, zorder=5)\n",
    "    ax.set(title=titulo, xlabel='Threshold', ylabel='Lucro Esperado (R$)')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'R${x:,.0f}'))\n",
    "\n",
    "plt.suptitle('Mesmo AUC â€” thresholds diferentes porque os custos de negÃ³cio sÃ£o diferentes',\n",
    "             fontweight='bold', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'CHURN  â†’ threshold Ã³timo: {t_ot_c:.2f} | EMP: R$ {emp_c:,.0f}')\n",
    "print(f'DEFAULT â†’ threshold Ã³timo: {t_ot_d:.2f} | EMP: R$ {emp_d:,.0f}')\n",
    "print('\\nğŸ’¡ A mÃ©trica certa depende do problema â€” nÃ£o existe uma resposta universal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coef_title",
   "metadata": {},
   "source": ["## 6. InterpretaÃ§Ã£o â€” Coeficientes e Odds Ratio"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Tabela de coeficientes + OR     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dica: use statsmodels.api para obter p-values\n",
    "# X_train_sm = sm.add_constant(X_train)\n",
    "# logit_sm = sm.Logit(y_train, X_train_sm).fit()\n",
    "\n",
    "X_cols = list(pd.get_dummies(\n",
    "    df.drop(columns=['id_cliente','segmento','prob_churn_real','reduziu_compras'],\n",
    "            errors='ignore'),\n",
    "    columns=['regiao','porte'], drop_first=True\n",
    ").columns)\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "logit_sm   = sm.Logit(y_train, X_train_sm).fit(disp=0)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'variavel':     ['intercepto'] + X_cols,\n",
    "    'coeficiente':  logit_sm.params.values,\n",
    "    'odds_ratio':   np.exp(logit_sm.params.values),\n",
    "    'p_value':      logit_sm.pvalues.values\n",
    "}).sort_values('odds_ratio', ascending=False)\n",
    "\n",
    "coef_df['significancia'] = pd.cut(\n",
    "    coef_df['p_value'],\n",
    "    bins=[-1, 0.001, 0.01, 0.05, 1],\n",
    "    labels=['***','**','*','ns']\n",
    ")\n",
    "print(coef_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "or_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: grÃ¡fico de Odds Ratio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sig_df = coef_df[coef_df['variavel'] != 'intercepto'].copy()\n",
    "sig_df = sig_df[sig_df['p_value'] < 0.05].sort_values('odds_ratio')\n",
    "\n",
    "cores_or = ['#c84b2f' if or_ > 1 else '#2f6ec8' for or_ in sig_df['odds_ratio']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, max(5, len(sig_df) * 0.38)))\n",
    "bars = ax.barh(sig_df['variavel'], sig_df['odds_ratio'] - 1,\n",
    "               left=1, color=cores_or, alpha=0.8)\n",
    "ax.axvline(1, color='#1a1a1a', lw=1.2, linestyle='--')\n",
    "for bar, val in zip(bars, sig_df['odds_ratio']):\n",
    "    ax.text(val + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.2f}Ã—', va='center', fontsize=9)\n",
    "ax.set(xlabel='Odds Ratio', title='Odds Ratio â€” VariÃ¡veis Significativas (p < 0.05)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nğŸ’¡ OR > 1 â†’ aumenta probabilidade de churn')\n",
    "print('   OR < 1 â†’ protege contra churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "action_title",
   "metadata": {},
   "source": ["## 7. SaÃ­da AcionÃ¡vel â€” Lista de Risco para o CRM"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPLETE AQUI â€” Gerar lista ranqueada por risco    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Dica: calcule a probabilidade para TODA a base (nÃ£o sÃ³ o teste)\n",
    "# Dica: use o threshold Ã³timo do EMP calculado acima (t_ot_c)\n",
    "\n",
    "# Reconstituir X completo com as mesmas transformaÃ§Ãµes\n",
    "X_full = pd.get_dummies(df.drop(\n",
    "    columns=['id_cliente','segmento','prob_churn_real','reduziu_compras'], errors='ignore'),\n",
    "    columns=['regiao','porte'], drop_first=True)\n",
    "X_full_imp = pd.DataFrame(imputer.transform(X_full), columns=X_full.columns)\n",
    "X_full_sc  = scaler.transform(X_full_imp)\n",
    "\n",
    "prob_full   = logit.predict_proba(X_full_sc)[:, 1]\n",
    "flag_risco  = (prob_full >= ___).astype(int)   # â† use t_ot_c\n",
    "\n",
    "lista_crm = pd.DataFrame({\n",
    "    'id_cliente':      df['id_cliente'],\n",
    "    'prob_churn':      prob_full.round(4),\n",
    "    'alerta':          flag_risco,\n",
    "    'segmento_A':      df['segmento_A'],\n",
    "    'nps_score':       df['nps_score'],\n",
    "    'canal_digital':   df['canal_digital'],\n",
    "}).sort_values('prob_churn', ascending=False)\n",
    "\n",
    "print(f'Clientes em alerta: {flag_risco.sum():,} ({flag_risco.mean():.1%} da base)')\n",
    "print('\\nTop 10 clientes prioritÃ¡rios:')\n",
    "lista_crm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: exportar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "lista_crm.to_csv('lista_risco_crm.csv', index=False)\n",
    "print('âœ… lista_risco_crm.csv exportada')\n",
    "print(f'   {flag_risco.sum():,} clientes para acionar')\n",
    "print(f'   ConcentraÃ§Ã£o Seg. A: {lista_crm[lista_crm.alerta==1].segmento_A.mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgb_compare_title",
   "metadata": {},
   "source": ["## 8. ComparaÃ§Ã£o: XGBoost captura o que o Logit nÃ£o vÃª\n\n> O Logit assume que cada variÃ¡vel tem efeito **linear** na log-odds. O XGBoost nÃ£o faz essa suposiÃ§Ã£o.  \n> O SHAP Dependence Plot vai mostrar que o NPS tem um efeito **nÃ£o-linear**: indiferente acima de 5, catastrÃ³fico abaixo."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgb_churn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ CÃ‰LULA PRONTA: XGBoost no churn (cÃ³digo completo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    import shap\n",
    "\n",
    "    xgb_clf = XGBClassifier(\n",
    "        n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, eval_metric='logloss', verbosity=0\n",
    "    )\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_xgb  = xgb_clf.predict(X_test)\n",
    "\n",
    "    auc_logit = roc_auc_score(y_test, y_proba_logit)\n",
    "    auc_xgb   = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "    print(f\"{'MÃ©trica':<20} {'Logit':>10} {'XGBoost':>10}\")\n",
    "    print('-' * 42)\n",
    "    print(f\"{'AUC-ROC':<20} {auc_logit:>10.3f} {auc_xgb:>10.3f}\")\n",
    "    print(f\"{'AcurÃ¡cia':<20} {accuracy_score(y_test, y_pred_logit):>10.3f} {accuracy_score(y_test, y_pred_xgb):>10.3f}\")\n",
    "    print(f\"{'F1-Score':<20} {f1_score(y_test, y_pred_logit):>10.3f} {f1_score(y_test, y_pred_xgb):>10.3f}\")\n",
    "\n",
    "    # SHAP dependence plot â€” NPS nÃ£o-linear\n",
    "    explainer   = shap.TreeExplainer(xgb_clf)\n",
    "    X_test_df   = pd.DataFrame(X_test, columns=X_cols)\n",
    "    shap_values = explainer.shap_values(X_test_df)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fpr_l, tpr_l, _ = roc_curve(y_test, y_proba_logit)\n",
    "    fpr_x, tpr_x, _ = roc_curve(y_test, y_proba_xgb)\n",
    "    axes[0].plot(fpr_l, tpr_l, label=f'Logit  (AUC={auc_logit:.3f})', color='#c84b2f', lw=2)\n",
    "    axes[0].plot(fpr_x, tpr_x, label=f'XGBoost (AUC={auc_xgb:.3f})', color='#2f6ec8', lw=2)\n",
    "    axes[0].plot([0,1],[0,1],'--', color='#888')\n",
    "    axes[0].set(title='Curvas ROC â€” mesmo problema', xlabel='FPR', ylabel='TPR')\n",
    "    axes[0].legend()\n",
    "\n",
    "    shap.dependence_plot('nps_score', shap_values, X_test_df, ax=axes[1], show=False)\n",
    "    axes[1].set_title('NPS Ã— SHAP: relaÃ§Ã£o nÃ£o-linear\\n(XGBoost vÃª; Logit nÃ£o)')\n",
    "    axes[1].axhline(0, color='#888', lw=0.8, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nğŸ’¡ Abaixo de NPS 5, o risco de churn acelera â€” relaÃ§Ã£o nÃ£o-linear.')\n",
    "    print('   Logit vÃª um coeficiente mÃ©dio. XGBoost captura o threshold crÃ­tico.')\n",
    "\n",
    "except ImportError:\n",
    "    print('âš ï¸  xgboost/shap nÃ£o instalados. Execute: !pip install xgboost shap')\n",
    "    print('   No Google Colab, ambos jÃ¡ estÃ£o disponÃ­veis.')"
   ]
  }
 ]
}
